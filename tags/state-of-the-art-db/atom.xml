<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh">
	<title>Fang Li - State-of-the-art DB</title>
	<subtitle>Fang Li&#x27;s Blog</subtitle>
	<link href="https://ffangli.github.io/tags/state-of-the-art-db/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="https://ffangli.github.io"/>
	<generator uri="https://www.getzola.org/">Zola</generator>
	<updated>2022-06-07T20:17:19+00:00</updated>
	<id>https://ffangli.github.io/tags/state-of-the-art-db/atom.xml</id>
	<entry xml:lang="zh">
		<title>Reading Notes on FEDB</title>
		<published>2022-06-07T20:17:19+00:00</published>
		<updated>2022-06-07T20:17:19+00:00</updated>
		<link rel="alternate" href="https://ffangli.github.io/202206072017/" type="text/html"/>
		<id>https://ffangli.github.io/202206072017/</id>
		<content type="html">&lt;p&gt;Optimizing In-memory Database Engine for AI-powered On-line Decision Augmentation Using Persistent Memory (VLDB 2021, 4paradigm, NUS, Intel)&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;FEDB is an open source in memory feature database from Fourth Paradigm.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;questions&quot;&gt;Questions&lt;&#x2F;h2&gt;
&lt;p&gt;Why do we need a special feature database?&lt;&#x2F;p&gt;
&lt;p&gt;The main reason mentioned in the paper is to solve the problem of slow online feature extraction (more on that below). This allows the feature designer to focus on the product level requirements (e.g. low latency, high concurrency, disaster recovery, high availability, scalability, smooth upgrades, monitorability, etc.) and use SQL for operations. This allows feature designers to focus on design and not be distracted by engineering efficiency.&lt;&#x2F;p&gt;
&lt;p&gt;The first thing we need to understand is what online feature extraction is, using the fraud detection example they gave: every time a credit card is swiped, it has to be determined within 10ms whether it is fraud or not. There are a few basic features (a few hundred), but there are many more real-time features (a few thousand) that can only be retrieved by querying the top three shops visited in the period before the card is swiped, and the most frequent purchases, etc. This &amp;quot;time before&amp;quot; is called a time window, and we can set the width of the window to a few minutes, hours, days, etc. The more time windows we have, the more features we can get, and the more accurate the fraud detection will be. The more time windows we have, the more features we get and the more accurate the fraud detection will be. As you can see, the acquisition of these features is indeed time consuming, and if the online application is time sensitive, the feature extraction time may be unacceptable. And this OLDA model of multiple summary queries after a single insert does not fit well with the typical workload of OLTP, OLAP, and time series databases.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;design&quot;&gt;Design&lt;&#x2F;h2&gt;
&lt;p&gt;They have designed a SQL-like language called FEQL (Feature Extraction Query Language) specifically for feature extraction, which allows the user to define multiple time windows at once without having to execute them multiple times. This EFQL is compiled using llvm for parsing optimisation. The lower storage engine tier is a typical high availability architecture.&lt;&#x2F;p&gt;
&lt;p&gt;As an optimization, FEDB can also use PMEM (persistent memory) to implement a double-tier jump table. For the first problem, the authors use PMDK and libpmemobj-cpp provided by Intel to request&#x2F;reclaim physical memory and maintain a pool of requested memory to build the jump table so that performance is not affected. For the second problem, EFDB uses the last 4 empty bits of the pointer to mark if it is dirty&#x2F;delete, and if it is dirty and you want to read it, you have to flush it first.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;resource&quot;&gt;Resource&lt;&#x2F;h2&gt;
&lt;p&gt;https:&#x2F;&#x2F;github.com&#x2F;4paradigm&#x2F;OpenMLDB&lt;&#x2F;p&gt;
&lt;p&gt;Chen, Cheng, et al. &amp;quot;Optimizing in-memory database engine for AI-powered on-line decision augmentation using persistent memory.&amp;quot; &lt;em&gt;Proceedings of the VLDB Endowment&lt;&#x2F;em&gt; 14.5 (2021): 799-812.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="zh">
		<title>Reading Notes on Data Blocks</title>
		<published>2022-05-22T20:17:19+00:00</published>
		<updated>2022-05-22T20:17:19+00:00</updated>
		<link rel="alternate" href="https://ffangli.github.io/202205222017/" type="text/html"/>
		<id>https://ffangli.github.io/202205222017/</id>
		<content type="html">&lt;p&gt;Data Blocks: Hybrid OLTP and OLAP on Compressed Storage using both Vectorization and Compilation (SIGMOD 2016, TUM)&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Data Blocks is a storage layer technology developed by the TUM database group to reduce the memory footprint of HyPer (OLTP + OLAP hybrid main memory database) without sacrificing existing performance.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;questions&quot;&gt;Questions&lt;&#x2F;h2&gt;
&lt;p&gt;To reduce the memory footprint, the data is divided into cold and hot parts, with the cold data being compressed to reduce memory pressure (and evicted out of memory) and the hot data not compressed. However, unlike some typical OLAP systems, HyPer does not use very high compression ratios, but rather restraint in the use of lightweight compression (byte-addressable compression formats), mainly for the sake of OLTP workoad performance (efficient point accesses). The SARGable scan restrictions (i.e., =, is, &amp;lt;, ≤, &amp;gt;, ≥, between) can be compared directly on this lightly compressed data. Query acceleration techniques such as vectorized execution for highly compressed columnar data are not suitable for OLTP, which generally does not require access to large amounts of data, but rather to locate data quickly.&lt;&#x2F;p&gt;
&lt;p&gt;As a remark, identifying hot and cold data is not the focus of this article; there are many off-the-shelf algorithms that solve this problem (e.g., Anti-Caching).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;&#x2F;h2&gt;
&lt;p&gt;This paper contributes to three main areas, the second of which is likely to be more widely understood.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Design of Data Blocks &#x2F;&#x2F; designed for cold data, Chunk compressed into Data Blocks&lt;&#x2F;li&gt;
&lt;li&gt;Lightweight indexing (PSMA) &#x2F;&#x2F; designed for Data Blocks&lt;&#x2F;li&gt;
&lt;li&gt;SIMD algorithm for accelerated predicate evaluation &#x2F;&#x2F; done on Data Block&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;design-of-data-blocks&quot;&gt;Design of Data Blocks&lt;&#x2F;h2&gt;
&lt;p&gt;When a chunk is considered cold, it is compressed into a read-optimized immutable Data Block (not writable). If the data on the cold Data Block is to be updated, the data on the Data Block is first deleted (marked with a flag) and a new one is inserted on top of the hot data. This ensures that OLTP performance is maintained.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;lightweight-indexing-designed-for-data-blocks&quot;&gt;Lightweight indexing designed for Data Blocks&lt;&#x2F;h2&gt;
&lt;p&gt;The Data Block layout is not much more than meta data + compressed data. What is special about the layout is that a lightweight index, PSMA (Positional SMAs), is stored for each column. Compared to traditional SMAs that only store an interval mix max, PSMAs further reduce the possible query intervals. That is, if a point look up falls within an SMA in this interval, the PSMA narrows down the data to be scanned instead of scanning the entire Data Block (for that query condition column) directly.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The PSMA query process is as follows.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;First a number v is subtracted from the SMA min in the data block interval of the column to obtain a delta value &lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The highest non-zero byte value of this delta value is , and the remaining number of bytes is r
then the index value &lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Query the look up table of the PSMA with i as the key, and get a range, which is the interval where the final result exists&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Understanding the query principle, it is easy to construct a PSMA by scanning through the data and continuously updating each range.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;&#x2F;h2&gt;
&lt;p&gt;Question: Can PSMA only be used to speed up point queries? If it is an OLAP range query, PSMA has to do multiple look up + union operations for each range, which is not necessarily cost effective.&lt;&#x2F;p&gt;
&lt;p&gt;Then there is the compression of Data Blocks. One of the main features is that each Data Block may have a different compression algorithm for each column, and they will choose the one that is best for their data (the one that minimises memory usage). As for specific compression algorithms, Data Blocks consider three algorithms that they believe balance compression ratio and query performance.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;single value compression &#x2F;&#x2F; if all values of an attribute in a block are equa, e.g., null&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;ordered dictionary compression &#x2F;&#x2F; the relative order of keys before compression is the same as after compression&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;truncation &#x2F;&#x2F; value - min, does not apply to double and string, string will always be compressed to integer&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;When a query comes in, first see if you can exclude the Data Block with SMA and PSMA or narrow the query, then use vectorized execution to compare directly on the compressed data.&lt;&#x2F;p&gt;
&lt;p&gt;However, the fact that each chunk is compressed differently poses a design challenge for the JIT tuple-at-a-time engine (just-in-time compilation of SQL queries directly into executable code). If the layout of each Data Block is different, the number of code paths to be generated by scan grows exponentially. To solve this problem, the authors took advantage of the vectorized scan remain interpreted and can be pre-compiled capability and combined it with JIT.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;resource&quot;&gt;Resource&lt;&#x2F;h2&gt;
&lt;p&gt;Harald Lang, Tobias Mühlbauer, Florian Funke, Peter A. Boncz, Thomas Neumann, and Alfons Kemper. 2016. Data Blocks: Hybrid OLTP and OLAP on Compressed Storage using both Vectorization and Compilation. In Proceedings of the 2016 International Conference on Management of Data (SIGMOD &#x27;16). Association for Computing Machinery, New York, NY, USA, 311–326. https:&#x2F;&#x2F;doi.org&#x2F;10.1145&#x2F;2882903.2882925&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="zh">
		<title>Reading Notes on Anti-Caching</title>
		<published>2022-04-02T20:17:19+00:00</published>
		<updated>2022-04-02T20:17:19+00:00</updated>
		<link rel="alternate" href="https://ffangli.github.io/202204022017/" type="text/html"/>
		<id>https://ffangli.github.io/202204022017/</id>
		<content type="html">&lt;p&gt;Anti-Caching: A New Approach to Database Management System Architecture (VLDB 2013, Brown + MIT)&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Anti-Caching is a data management mechanism designed by the H-Store project team for all-memory databases and is expected to solve the problem of physical memory limitations.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;questions&quot;&gt;Questions&lt;&#x2F;h2&gt;
&lt;p&gt;The reason for the redesign is that the previous disk-based BufferManager had an excessive overhead in full memory scenarios (if the page is already in main memory, finding the corresponding memory pointer based on the page identifier is completely useless, according to a According to a SIGMOD 2018 paper, 1&#x2F;3 of CPU cycles are wasted here). For this reason, many pure memory databases have abandoned BufferManager (e.g. H-Store, VoltDB [the commercial version of H-Store], MemSQL, and RAMCloud). The problem was that when the amount of physical memory was exceeded, pure in-memory databases started to experience page faults, which suddenly and severely slowed down the system. So at the time, pure memory databases were advising users not to manage more data than was available in physical memory. To solve this problem, the H-Store project team designed Anti-Caching.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;design-goals&quot;&gt;Design goals&lt;&#x2F;h2&gt;
&lt;p&gt;When physical memory is about to be exhausted, the database collects cold data and writes it to disk in a cost effective manner.&lt;&#x2F;p&gt;
&lt;p&gt;Unlike previous cache mechanisms, data does not exist in both hard disk and memory, but only in one of these places. Because memory is the primary carrier of data, the typical scenario is that data starts in memory and then cold data is expelled to the hard disk (rather than data starting on the hard disk and hot data being cached to memory). This approach is actually somewhat similar to virtual memory, except that memory is now managed by the database rather than the operating system, which provides more precise control (e.g., tuple level eviction).&lt;&#x2F;p&gt;
&lt;p&gt;To understand the design of Anti-Caching it may be useful to review the key features of the H-Stored pure memory database: the H-Store is single-transaction (multiple physical nodes, multiple partitions per node, one thread per partition responsible for executing queries involving that partition), so it is lock-free. This is fine when the query mostly involves only one partition), so there is no locking. In terms of persistence, snapshots are flushed periodically, and command logs are also flushed (irregularly, or less than a max period, I guess) (after the group, to reduce the number of I&#x2F;Os), and only after the command log has been dropped does the commit return successful.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;design&quot;&gt;Design&lt;&#x2F;h2&gt;
&lt;p&gt;The core operation for implementing Anti-Caching is a two-step process.&lt;&#x2F;p&gt;
&lt;p&gt;Aggregate the tuple of cold data to be expelled from memory (according to the LRU) into a block.
Update an in-memory table (evicted table) to keep track of which tuples have been written to disk.
And, because H-Stroe is single-transaction, maintaining these data structures does not require chaining.&lt;&#x2F;p&gt;
&lt;p&gt;All that remains to be solved is some auxiliary data structures and how to do this elegantly. For example, in addition to the evicted table, there will be a block table to maintain all blocks. To facilitate access to cold data, a LRU Chain is maintained (for each table), so that when a tuple is accessed, it is retrieved directly from its original location and placed at the end of the chain. When implemented, the pointer to this bi-linked table is placed directly in the tuple header to reduce memory usage. To reduce CPU pressure, only some transactions are selected to update the LRU information (sampling). In addition to the data itself, the block table and evicted table are also written to disk during persistence.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;&#x2F;h2&gt;
&lt;p&gt;Question: Why do we need block tables?
My understanding is that if only evicted tables were used, then each tuple would have to hold specific block location information, which would undoubtedly increase the memory footprint and undermine the design goal of expelling tuples to reduce memory pressure. Using additional block tables in a dictionary-like fashion can further compress memory usage.&lt;&#x2F;p&gt;
&lt;p&gt;Another trade-off is that the tuple from the block has to be merged back into the in-memory table structure, but should it be just this data? Or should the whole block be merged? If the former, the blocks on disk will need to be tidied up regularly, otherwise there will be more and more holes. If the latter, it may increase the memory load and the number of evicts.&lt;&#x2F;p&gt;
&lt;p&gt;The experiments were compared with MySQL, MySQL + Memcached, using the YCSB (Yahoo Cloud Serving Benchmark) and TPC-C (OLTP) datasets. The basic conclusion is that the more skewed the query the more pronounced the Anti-Caching effect. After all, most of the hot pages used can be kept in memory at this point.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;resource&quot;&gt;Resource&lt;&#x2F;h2&gt;
&lt;p&gt;Justin DeBrabant, Andrew Pavlo, Stephen Tu, Michael Stonebraker, and Stan Zdonik. 2013. Anti-caching: a new approach to database management system architecture. Proc. VLDB Endow. 6, 14 (September 2013), 1942–1953. https:&#x2F;&#x2F;doi.org&#x2F;10.14778&#x2F;2556549.2556575&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="zh">
		<title>Reading Notes on LeanStore</title>
		<published>2022-03-13T20:17:19+00:00</published>
		<updated>2022-03-13T20:17:19+00:00</updated>
		<link rel="alternate" href="https://ffangli.github.io/202203132017/" type="text/html"/>
		<id>https://ffangli.github.io/202203132017/</id>
		<content type="html">&lt;p&gt;LeanStore: In-Memory Data Management Beyond Main Memory (ICDE 2018, TUM)&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;LeanStore is a database designed by the TUM database group after HyPer&#x27;s pure in-memory database, which favours large in-memory scenarios but provides better management of ultra-physical in-memory data volumes than a pure in-memory database.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;questions&quot;&gt;Questions&lt;&#x2F;h2&gt;
&lt;p&gt;Why go &amp;quot;backwards&amp;quot; from the memory-only databases of the last few years to databases using hard disks? It&#x27;s really a question of price&#x2F;performance, 20 years ago when memory-only databases were being talked about, memory capacity was soaring and prices were plummeting, but that trend, like CPUs, has almost stalled in the last 10 years. Memory prices today are only half as cheap as they were a decade ago, but hard drives or SSDs are dropping&#x2F;increasing in price at a rate that is visible to the naked eye. Mainstream SSDs can now achieve a bandwidth gap that is only an order of magnitude behind memory (a few GB&#x2F;s vs. tens of GB&#x2F;s). Also databases actually have a lot of cold data in them, which would be really wasteful to keep in memory as well. But previous in-memory-only databases have always had limitations or severe performance degradation when it comes to external storage, or mechanisms that were too complex to move from the lab to production systems (such as the Anti-Caching design of H-Store, whose indexes contain hot and cold data and the indexes themselves have to be in memory, and given that index sizes often take up half the H-Store often encounters significant memory pressure). For this reason, TUM has designed LeanStore to solve these problems.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;design-goals&quot;&gt;Design goals&lt;&#x2F;h2&gt;
&lt;ol&gt;
&lt;li&gt;Performs as well as an in-memory-only database when the amount of data is less than physical memory&lt;&#x2F;li&gt;
&lt;li&gt;Better than in-memory databases when the amount of data is greater than physical memory&lt;&#x2F;li&gt;
&lt;li&gt;Better than traditional databases in any scenario (using a typical BufferManager, such as BerkeleyDB)&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;In order to achieve these benefits, LeanStore has adopted the following design：&lt;&#x2F;p&gt;
&lt;h2 id=&quot;design&quot;&gt;Design&lt;&#x2F;h2&gt;
&lt;ol&gt;
&lt;li&gt;In a pure memory scenario, using a traditional BufferManager would often result in severe overhead and critical path bottlenecks (e.g. querying the hash table to find the in memory pointer corresponding to the page identifier, which is is an overhead because what&#x27;s the point of mapping when it&#x27;s pure memory). As a result, almost all in-memory databases (e.g., HyPer, HANA, H-Store, Hekaton, and Silo) have abandoned BufferManager. However, BufferManager still has many benefits for traditional hard disk databases, such as the ease of managing the amount of data that exceeds memory and the reduction of disk I&#x2F;O, etc. To facilitate the management of out-of-memory data, LeanStore still uses BufferManager, but with a number of optimisations to reduce the overhead in purely in-memory scenarios.&lt;&#x2F;li&gt;
&lt;li&gt;A more lightweight page replacement mechanism has been designed to get cold data out of memory when necessary. Traditional disk-based database page replacement policies (such as LRU and Clock) need to keep track of the number of accesses to each page, etc., which can sometimes become a bottleneck when updating access data for some frequently accessed pages. If you still use these disk-based db policies, you will inevitably be slower than a memory-only database in a memory-only scenario.&lt;&#x2F;li&gt;
&lt;li&gt;In terms of concurrency control, a similar approach to optimistic locking is used, an example of which is the Optimistic Lock Coupling on the Adaptive Radix Tree mentioned in the article.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;resource&quot;&gt;Resource&lt;&#x2F;h2&gt;
&lt;p&gt;V. Leis, M. Haubenschild, A. Kemper and T. Neumann, &amp;quot;LeanStore: In-Memory Data Management beyond Main Memory,&amp;quot; 2018 IEEE 34th International Conference on Data Engineering (ICDE), 2018, pp. 185-196, doi: 10.1109&#x2F;ICDE.2018.00026.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="zh">
		<title>Reading Notes on umbra</title>
		<published>2022-03-09T20:17:19+00:00</published>
		<updated>2022-03-09T20:17:19+00:00</updated>
		<link rel="alternate" href="https://ffangli.github.io/202203092017/" type="text/html"/>
		<id>https://ffangli.github.io/202203092017/</id>
		<content type="html">&lt;p&gt;Umbra DB&lt;&#x2F;p&gt;
&lt;p&gt;Umbra: A Disk-Based System with In-Memory Performance (CIDR 2020, TUM)&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;h2 id=&quot;basic-info&quot;&gt;Basic Info&lt;&#x2F;h2&gt;
&lt;p&gt;Umbra is a database designed by the TUM database group after the HyPer in-memory-only database, which is biased towards large memory scenarios but offers better management of supra-physical memory data volumes than the in-memory-only database. Umbra&#x27;s positioning overlaps with LeanStore and continues many of the LeanStore&#x27;s designs (e.g. pointer swizzling, page replacement strategy, optimistic latching...) It also continues much of the design of the LeanStore (e.g. pointer swizzling, page replacement strategy, optimistic latching...) but adds variable-size pages to the LeanStore to better support arbitrary data sizes (especially large data objects) without compromising performance. TUM therefore considers Umbra to be the true successor of Hyper (Umbra is the spiritual successor of our pure in-memory system HyPer).&lt;&#x2F;p&gt;
&lt;p&gt;Here are some details on the design of the variable-size page.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;variable-size-page&quot;&gt;Variable-size Page&lt;&#x2F;h2&gt;
&lt;p&gt;Umbra&#x27;s page starts at 64KB and is divided into multiple size classes. Theoretically, the page size can be up to the size of the entire buffer pool. Unlike previous variable size page buffer pool designs, Umbra does not need to set the size of the buffer pool memory available for each size class, so it can be used (api) much differently than a normal buffer manager.&lt;&#x2F;p&gt;
&lt;p&gt;However, supporting multiple page sizes can easily lead to fragmentation, and Umbra solves this problem by using a mapping between virtual address and physical memory (although a continuous virtual address may be discrete on physical memory). ). Each virtual memory region is sliced into chunks of the size of that size class, so that at least one of the virtual memory chunks is not allocated. Each virtual memory region is sliced into chunks of that size class, so there is no fragmentation, at least on the virtual memory. When a page is read, a physical memory mapping is created with pread. When a page is evict, a pwrite is written back and the MADV_DONTNEED flag is passed to the madvise system call to immediately release the physical The&lt;&#x2F;p&gt;
&lt;p&gt;Umbra&#x27;s relation is organised in a B+ tree (using synthetic key), all internal nodes are 64KB pages, and the leaf node can be a variable size page. DataBlock.&lt;&#x2F;p&gt;
&lt;p&gt;In addition to the variable size page, Umbra has made some additional minor optimisations compared to LeanStore, such as shared latching (to reduce the validation failure of optimistic latching in the case of read heavy workload + few writes) failure). Compared to HyPer, changes have been made to string handling (below), statistics collection (online reservoir sampling variant + HyperLogLog variant), and query compilation (pipelines are further disassembled into steps, abandoned the more generic llvm, wrote a more customised lightweight IR of its own, and adapted it for implementation (ICDE2018)).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;resource&quot;&gt;Resource&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;umbra-db.com&#x2F;&quot;&gt;Umbra&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
</content>
	</entry>
</feed>
